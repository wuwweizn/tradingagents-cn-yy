"""
é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹ (DashScope) é€‚é…å™¨
ä¸º TradingAgents æä¾›é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹çš„ LangChain å…¼å®¹æ¥å£
"""

import os
import json
from typing import Any, Dict, List, Optional, Union, Iterator, AsyncIterator, Sequence
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks.manager import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun
from langchain_core.tools import BaseTool
from langchain_core.utils.function_calling import convert_to_openai_tool
from pydantic import Field, SecretStr
import dashscope
from dashscope import Generation
from ..config.config_manager import token_tracker

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')



class ChatDashScope(BaseChatModel):
    """é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹çš„ LangChain é€‚é…å™¨"""
    
    # æ¨¡å‹é…ç½®
    model: str = Field(default="qwen-turbo", description="DashScope æ¨¡å‹åç§°")
    api_key: Optional[SecretStr] = Field(default=None, description="DashScope API å¯†é’¥")
    temperature: float = Field(default=0.1, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=2000, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    top_p: float = Field(default=0.9, description="æ ¸é‡‡æ ·å‚æ•°")
    
    # å†…éƒ¨å±æ€§
    _client: Any = None
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ– DashScope å®¢æˆ·ç«¯"""
        super().__init__(**kwargs)
        
        # è®¾ç½®APIå¯†é’¥
        api_key = self.api_key
        if api_key is None:
            api_key = os.getenv("DASHSCOPE_API_KEY")
        
        if api_key is None:
            raise ValueError(
                "DashScope API key not found. Please set DASHSCOPE_API_KEY environment variable "
                "or pass api_key parameter."
            )
        
        # é…ç½® DashScope
        if isinstance(api_key, SecretStr):
            dashscope.api_key = api_key.get_secret_value()
        else:
            dashscope.api_key = api_key
    
    @property
    def _llm_type(self) -> str:
        """è¿”å›LLMç±»å‹"""
        return "dashscope"
    
    def _convert_messages_to_dashscope_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """å°† LangChain æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º DashScope æ ¼å¼"""
        dashscope_messages = []
        
        for message in messages:
            if isinstance(message, SystemMessage):
                role = "system"
            elif isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, AIMessage):
                role = "assistant"
            else:
                # é»˜è®¤ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
                role = "user"
            
            content = message.content
            if isinstance(content, list):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œç›®å‰åªæå–æ–‡æœ¬
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                content = text_content
            
            dashscope_messages.append({
                "role": role,
                "content": str(content)
            })
        
        return dashscope_messages
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”ŸæˆèŠå¤©å›å¤"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        dashscope_messages = self._convert_messages_to_dashscope_format(messages)
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "model": self.model,
            "messages": dashscope_messages,
            "result_format": "message",
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
        }
        
        # æ·»åŠ åœæ­¢è¯
        if stop:
            request_params["stop"] = stop
        
        # åˆå¹¶é¢å¤–å‚æ•°
        request_params.update(kwargs)
        
        try:
            # è°ƒç”¨ DashScope API
            response = Generation.call(**request_params)
            
            if response.status_code == 200:
                # è§£æå“åº”
                output = response.output
                message_content = output.choices[0].message.content
                
                # æå–tokenä½¿ç”¨é‡ä¿¡æ¯
                input_tokens = 0
                output_tokens = 0
                
                # DashScope APIå“åº”ä¸­åŒ…å«usageä¿¡æ¯
                if hasattr(response, 'usage') and response.usage:
                    usage = response.usage
                    # æ ¹æ®APIæ–‡æ¡£ï¼Œusageå¯èƒ½åŒ…å«input_tokenså’Œoutput_tokens
                    if hasattr(usage, 'input_tokens'):
                        input_tokens = usage.input_tokens
                    if hasattr(usage, 'output_tokens'):
                        output_tokens = usage.output_tokens
                    # æœ‰äº›æƒ…å†µä¸‹å¯èƒ½æ˜¯total_tokens
                    elif hasattr(usage, 'total_tokens'):
                        # ä¼°ç®—è¾“å…¥å’Œè¾“å‡ºtokenï¼ˆå¦‚æœæ²¡æœ‰åˆ†åˆ«æä¾›ï¼‰
                        total_tokens = usage.total_tokens
                        # ç®€å•ä¼°ç®—ï¼šå‡è®¾è¾“å…¥å 30%ï¼Œè¾“å‡ºå 70%
                        input_tokens = int(total_tokens * 0.3)
                        output_tokens = int(total_tokens * 0.7)
                
                # è®°å½•tokenä½¿ç”¨é‡
                if input_tokens > 0 or output_tokens > 0:
                    try:
                        # ç”Ÿæˆä¼šè¯IDï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
                        session_id = kwargs.get('session_id', f"dashscope_{hash(str(messages))%10000}")
                        analysis_type = kwargs.get('analysis_type', 'stock_analysis')
                        
                        # ä½¿ç”¨TokenTrackerè®°å½•ä½¿ç”¨é‡
                        token_tracker.track_usage(
                            provider="dashscope",
                            model_name=self.model,
                            input_tokens=input_tokens,
                            output_tokens=output_tokens,
                            session_id=session_id,
                            analysis_type=analysis_type
                        )
                    except Exception as track_error:
                        # è®°å½•å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»è¦åŠŸèƒ½
                        logger.info(f"Token tracking failed: {track_error}")
                
                # åˆ›å»º AI æ¶ˆæ¯
                ai_message = AIMessage(content=message_content)
                
                # åˆ›å»ºç”Ÿæˆç»“æœ
                generation = ChatGeneration(message=ai_message)
                
                return ChatResult(generations=[generation])
            else:
                raise Exception(f"DashScope API error: {response.code} - {response.message}")
                
        except Exception as e:
            raise Exception(f"Error calling DashScope API: {str(e)}")
    
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """å¼‚æ­¥ç”ŸæˆèŠå¤©å›å¤"""
        # ç›®å‰ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œåç»­å¯ä»¥å®ç°çœŸæ­£çš„å¼‚æ­¥
        return self._generate(messages, stop, run_manager, **kwargs)
    
    def bind_tools(
        self,
        tools: Sequence[Union[Dict[str, Any], type, BaseTool]],
        **kwargs: Any,
    ) -> "ChatDashScope":
        """ç»‘å®šå·¥å…·åˆ°æ¨¡å‹"""
        # æ³¨æ„ï¼šDashScope ç›®å‰ä¸ç›´æ¥æ”¯æŒå·¥å…·è°ƒç”¨
        # è¿™é‡Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œä½†å®é™…ä¸Šå·¥å…·è°ƒç”¨éœ€è¦åœ¨åº”ç”¨å±‚å¤„ç†
        formatted_tools = []
        for tool in tools:
            if hasattr(tool, "name") and hasattr(tool, "description"):
                # è¿™æ˜¯ä¸€ä¸ª BaseTool å®ä¾‹
                formatted_tools.append({
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": getattr(tool, "args_schema", {})
                })
            elif isinstance(tool, dict):
                formatted_tools.append(tool)
            else:
                # å°è¯•è½¬æ¢ä¸º OpenAI å·¥å…·æ ¼å¼
                try:
                    openai_tool = convert_to_openai_tool(tool)
                    formatted_tools.append(openai_tool)
                    logger.debug(f"âœ… å·¥å…·è½¬æ¢æˆåŠŸ: {getattr(tool, 'name', 'unknown')}")
                except Exception as e:
                    # è®°å½•é”™è¯¯å¹¶æä¾›å›é€€æœºåˆ¶
                    tool_name = getattr(tool, 'name', 'unknown')
                    logger.warning(f"âš ï¸ å·¥å…·è½¬æ¢å¤±è´¥: {tool_name} - {e}")
                    
                    # å°è¯•æ‰‹åŠ¨åˆ›å»ºåŸºæœ¬å·¥å…·æ ¼å¼ä½œä¸ºå›é€€
                    try:
                        fallback_tool = {
                            "type": "function",
                            "function": {
                                "name": tool_name,
                                "description": getattr(tool, 'description', f'å·¥å…·: {tool_name}'),
                                "parameters": {
                                    "type": "object",
                                    "properties": {},
                                    "required": []
                                }
                            }
                        }
                        formatted_tools.append(fallback_tool)
                        logger.info(f"ğŸ”„ ä½¿ç”¨å›é€€å·¥å…·æ ¼å¼: {tool_name}")
                    except Exception as fallback_error:
                        logger.error(f"âŒ å›é€€å·¥å…·æ ¼å¼åˆ›å»ºå¤±è´¥: {tool_name} - {fallback_error}")
                        # å¦‚æœå›é€€ä¹Ÿå¤±è´¥ï¼Œè‡³å°‘è®°å½•è­¦å‘Šï¼Œä¸é™é»˜å¤±è´¥

        # åˆ›å»ºæ–°å®ä¾‹ï¼Œä¿å­˜å·¥å…·ä¿¡æ¯
        new_instance = self.__class__(
            model=self.model,
            api_key=self.api_key,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            **kwargs
        )
        new_instance._tools = formatted_tools
        return new_instance

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›æ ‡è¯†å‚æ•°"""
        return {
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
        }


# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
DASHSCOPE_MODELS = {
    # é€šä¹‰åƒé—®ç³»åˆ—
    "qwen-turbo": {
        "description": "é€šä¹‰åƒé—® Turbo - å¿«é€Ÿå“åº”ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯",
        "context_length": 8192,
        "recommended_for": ["å¿«é€Ÿä»»åŠ¡", "æ—¥å¸¸å¯¹è¯", "ç®€å•åˆ†æ"]
    },
    "qwen-plus": {
        "description": "é€šä¹‰åƒé—® Plus - å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
        "context_length": 32768,
        "recommended_for": ["å¤æ‚åˆ†æ", "ä¸“ä¸šä»»åŠ¡", "æ·±åº¦æ€è€ƒ"]
    },
    "qwen-max": {
        "description": "é€šä¹‰åƒé—® Max - æœ€å¼ºæ€§èƒ½",
        "context_length": 32768,
        "recommended_for": ["æœ€å¤æ‚ä»»åŠ¡", "ä¸“ä¸šåˆ†æ", "é«˜è´¨é‡è¾“å‡º"]
    },
    "qwen-max-longcontext": {
        "description": "é€šä¹‰åƒé—® Max é•¿æ–‡æœ¬ç‰ˆ - æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡",
        "context_length": 1000000,
        "recommended_for": ["é•¿æ–‡æ¡£åˆ†æ", "å¤§é‡æ•°æ®å¤„ç†", "å¤æ‚æ¨ç†"]
    },
}


def get_available_models() -> Dict[str, Dict[str, Any]]:
    """è·å–å¯ç”¨çš„ DashScope æ¨¡å‹åˆ—è¡¨"""
    return DASHSCOPE_MODELS


def create_dashscope_llm(
    model: str = "qwen-plus",
    api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: int = 2000,
    **kwargs
) -> ChatDashScope:
    """åˆ›å»º DashScope LLM å®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    
    return ChatDashScope(
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )
